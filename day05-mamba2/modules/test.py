import torch
# 1. ç›´æ¥å¯¼å…¥å®˜æ–¹å®‰è£…çš„ Mamba2 æ¨¡å—
import mamba2

def debug_run():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Running on: {device}")

    # 2. åˆå§‹åŒ–å®˜æ–¹æ¨¡å‹
    # æ ¸å¿ƒå…³é”®ç‚¹ï¼šuse_mem_eff_path=False
    # è¿™ä¼šå¼ºåˆ¶å®˜æ–¹æºç èµ° else åˆ†æ”¯ï¼Œè€Œä¸æ˜¯å»è·‘é»‘ç›’ Kernel
    model = mamba2.Mamba2Simple(
        d_model=256,
        use_mem_eff_path=False 
    ).to(device)

    # 3. é€ æ•°æ®
    # batch=1, seq_len=128, d_model=256
    x = torch.linspace(0, 1, 256).to(device)
    x = x.unsqueeze(0).unsqueeze(0).repeat(1, 128, 1)
    print(x.shape)

    # x = torch.randn(1, 128, 256).to(device)

    print("ğŸš€ å¼€å§‹è¿è¡Œ... è¯·ç¡®ä¿ä½ å·²ç»åœ¨å®˜æ–¹æºç æ–‡ä»¶é‡Œæ‰“å¥½äº†æ–­ç‚¹ï¼")
    y = model(x) 
    print(y[0, :5, :10])

if __name__ == "__main__":
    debug_run()