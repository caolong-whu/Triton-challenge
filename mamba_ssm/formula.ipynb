{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72593e2f",
   "metadata": {},
   "source": [
    "## 1. 前向传播\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7807fa",
   "metadata": {},
   "source": [
    "$h_t = A_th_{t-1} + B_tx_t$\n",
    "\n",
    "$y_t = C_th_t$\n",
    "\n",
    "$y_t = C_t(A_th_{t-1} + B_tx_t) = C_t \\times h_{old} + C_tB_tx_t$\n",
    "\n",
    "假设输入为 $X = [x_0, x_1, x_2, x_3, x_4, x_5, x_6, x_7]$， 输出为$Y = [y_0, y_1, y_2, y_3, y_4, y_5, y_6, y_7]$\n",
    "\n",
    "则，$Y = MX$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5096fd4",
   "metadata": {},
   "source": [
    "$M = \\left[\n",
    "\\begin{array}{lll|lll|lll}\n",
    "  C_0^T A_{0:0} B_0 & & & & & & & & \\\\\n",
    "  C_1^T A_{1:0} B_0 & C_1^T A_{1:1} B_1 & & & & & & & \\\\\n",
    "  C_2^T A_{2:0} B_0 & C_2^T A_{2:1} B_1 & C_2^T A_{2:2} B_2 & & & & & & \\\\\n",
    "  \\hline\n",
    "  C_3^T A_{3:0} B_0 & C_3^T A_{3:1} B_1 & C_3^T A_{3:2} B_2 & C_3^T A_{3:3} B_3 & & & & & \\\\\n",
    "  C_4^T A_{4:0} B_0 & C_4^T A_{4:1} B_1 & C_4^T A_{4:2} B_2 & C_4^T A_{4:3} B_3 & C_4^T A_{4:4} B_4 & & & & \\\\\n",
    "  C_5^T A_{5:0} B_0 & C_5^T A_{5:1} B_1 & C_5^T A_{5:2} B_2 & C_5^T A_{5:3} B_3 & C_5^T A_{5:4} B_4 & C_5^T A_{5:5} B_5 & & & \\\\\n",
    "  \\hline\n",
    "  C_6^T A_{6:0} B_0 & C_6^T A_{6:1} B_1 & C_6^T A_{6:2} B_2 & C_6^T A_{6:3} B_3 & C_6^T A_{6:4} B_4 & C_6^T A_{6:5} B_5 & C_6^T A_{6:6} B_6 & & \\\\\n",
    "  C_7^T A_{7:0} B_0 & C_7^T A_{7:1} B_1 & C_7^T A_{7:2} B_2 & C_7^T A_{7:3} B_3 & C_7^T A_{7:4} B_4 & C_7^T A_{7:5} B_5 & C_7^T A_{7:6} B_6 & C_7^T A_{7:7} B_7 & \\\\\n",
    "  C_8^T A_{8:0} B_0 & C_8^T A_{8:1} B_1 & C_8^T A_{8:2} B_2 & C_8^T A_{8:3} B_3 & C_8^T A_{8:4} B_4 & C_8^T A_{8:5} B_5 & C_8^T A_{8:6} B_6 & C_8^T A_{8:7} B_7 & C_8^T A_{8:8} B_8\n",
    "\\end{array}\n",
    "\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0e0385",
   "metadata": {},
   "source": [
    "对于$M$中的任意元素$M_{ij}$，代表的是$x_j$对$y_i$的贡献，通项公式为：$M_{ij}=C_i \\times A_{i:j} \\times B_j$，其中$A_{i:j}= \\prod_{k=j+1}^{i} A_{k}，{j+1<i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b3b789",
   "metadata": {},
   "source": [
    "对于$M$矩阵中的元素，有两种类型，一种是位于对角线上的元素，即$M_{ii}=C_iB_i$，代表$x_t$对当前输出$y_t$的贡献"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64b3c84",
   "metadata": {},
   "source": [
    "第二种是位于下三角的元素，即$M_{ij}, i>j$，代表历史$x_j$对当前$y_i$的贡献"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515f9d4b",
   "metadata": {},
   "source": [
    "采用分块思想，将输入分成多个chunks，假如分成3个chunks，chunk_size为3，那么M矩阵就是一个$3\\times3$的分块矩阵，位于对角线区域的块，分别为$M_{00}, M_{11}, M_{22}$，计算公式为\n",
    "\n",
    "元素形式：$M_{ii}[t, k] = \\begin{cases} \n",
    "C_t \\cdot B_k^\\top \\cdot \\underbrace{e^{L_t - L_k}}_{\\text{Decay}} & \\text{if } t \\ge k \\\\\n",
    "0 & \\text{if } t < k \n",
    "\\end{cases}$\n",
    "\n",
    "矩阵形式：$M_{ii} = \\underbrace{\\left( C^{(i)} {B^{(i)}}^\\top \\right)}_{\\text{Attention Scores}} \\odot \\underbrace{\\mathbf{D}^{(i)}_{\\text{causal}}}_{\\text{Mask \\& Decay}}$ 其中衰减矩阵是一个下三角矩阵：$\\mathbf{D}^{(i)}_{\\text{causal}}[t, k] = \\begin{cases} e^{L_t - L_k} & t \\ge k \\\\ 0 & t < k \\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16c83f5",
   "metadata": {},
   "source": [
    "而位于下三角区域的块，$M_{10}, M_{20}, M_{21}$，计算公式为"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71135e7",
   "metadata": {},
   "source": [
    "元素形式：$M_{ij}[t, k] = \\underbrace{\\left( C_t \\cdot e^{L_t - L_{\\text{start}}^{(i)}} \\right)}_{\\text{Left (Decompression)}} \\times \\underbrace{\\exp\\left(L_{\\text{start}}^{(i)} - L_{\\text{end}}^{(j)}\\right)}_{\\text{Middle (Passing)}} \\times \\underbrace{\\left( B_k \\cdot e^{L_{\\text{end}}^{(j)} - L_k} \\right)}_{\\text{Right (Compression)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2dc4f2",
   "metadata": {},
   "source": [
    "矩阵形式：$M_{ij} = \\underbrace{\\left( C^{(i)} \\odot \\mathbf{d}_{L}^{(i)} \\right)}_{\\mathbf{L}_i} \\cdot A_{j \\to i} \\cdot \\underbrace{\\left( B^{(j)} \\odot \\mathbf{d}_{R}^{(j)} \\right)^\\top}_{\\mathbf{R}_j^\\top}$ 或者 $M_{ij} = \\left[ \\text{diag}\\left(e^{L_{t} - L_{\\text{start}}^{(i)}}\\right) C^{(i)} \\right] \\cdot e^{L_{\\text{start}}^{(i)} - L_{\\text{end}}^{(j)}} \\cdot \\left[ \\text{diag}\\left(e^{L_{\\text{end}}^{(j)} - L_{k}}\\right) B^{(j)} \\right]^\\top$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e34a8eb",
   "metadata": {},
   "source": [
    "## 2. 后向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7c7f84",
   "metadata": {},
   "source": [
    "### 1 计算$dz, dSSM, dD, ddAcs$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205cc635",
   "metadata": {},
   "source": [
    "$O = y \\odot z \\odot \\sigma(z) + D \\cdot x$\n",
    "\n",
    "1. dz\n",
    "\n",
    "$dz = \\delta O \\odot y \\odot \\sigma(z) \\left( 1 + z (1 - \\sigma(z)) \\right)$\n",
    "\n",
    "2. dD\n",
    "\n",
    "$dD = \\sum_{b, t} \\left( \\delta O_{b,t} \\odot x_{b,t} \\right)$\n",
    "\n",
    "3. dy (用于继续进行后向传播)\n",
    "\n",
    "$\\delta y = \\delta O \\odot \\text{SiLU}(z)$\n",
    "\n",
    "4. ddA_cs\n",
    "\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial L_t} = \\sum_{p} \\left( \\delta y_{t,p} \\cdot y_{t,p} \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e46987",
   "metadata": {},
   "source": [
    "### 2 计算梯度\n",
    "\n",
    "### 2.1 经过第一步，我们已经得到了SSM的梯度$\\partial y$，SSM的计算分为非对角线区域和对角线区域两种情况，先看非对角线区域，其前向传播计算公式为：\n",
    "\n",
    "$M_{ij}[t, k] = \\underbrace{\\left( C_t \\cdot e^{L_t - L_{\\text{start}}^{(i)}} \\right)}_{\\text{Left (Decompression)}} \\times \\underbrace{\\exp\\left(L_{\\text{start}}^{(i)} - L_{\\text{end}}^{(j)}\\right)}_{\\text{Middle (Passing)}} \\times \\underbrace{\\left( B_k \\cdot e^{L_{\\text{end}}^{(j)} - L_k} \\right)}_{\\text{Right (Compression)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ed749c",
   "metadata": {},
   "source": [
    "按照反向传播的顺序，最后计算的是左侧部分，那么反向传播就要先计算左侧部分的梯度，左项可以化简为：$$y_{t,p} = \\underbrace{\\text{Decay}_t}_{\\text{标量}} \\cdot \\sum_{n=1}^{N} (\\underbrace{h_{p,n}}_{\\text{状态}} \\cdot \\underbrace{C_{t,n}}_{\\text{参数}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43243ad",
   "metadata": {},
   "source": [
    "那么其梯度公式为："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0193621c",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial \\mathcal{L}}{\\partial h_{p,n}} = \\sum_{t=0}^{Q-1} \\left( \\frac{\\partial \\mathcal{L}}{\\partial y_{t,p}} \\cdot \\frac{\\partial y_{t,p}}{\\partial h_{p,n}} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec72bbc1",
   "metadata": {},
   "source": [
    "因为 $h_{p,n}$ 参与了 $t=0, 1, \\dots, Q-1$ 所有时刻的计算，所以每个时刻的 Loss 都要对它负责。\n",
    "\n",
    "$$dh_{p,n} = \\sum_{t=0}^{Q-1} (\\text{dout}_{t,p} \\cdot \\text{Decay}_t) \\cdot C_{t,n}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e748fae2",
   "metadata": {},
   "source": [
    "关于形状变化：\n",
    "\n",
    "定义加权后的梯度 $\\tilde{D}$：$$\\tilde{D}_{t,p} = \\text{dout}_{t,p} \\cdot \\text{Decay}_t$$\n",
    "\n",
    "形状：[Q, P] (Time, Headdim)现在公式变成了：$$dh_{p,n} = \\sum_{t=0}^{Q-1} \\tilde{D}_{t,p} \\cdot C_{t,n}$$\n",
    "\n",
    "而C的形状为[Q, N]\n",
    "\n",
    "消去Q维度，最终的维度正好是$[P, N]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b93f13f",
   "metadata": {},
   "source": [
    "### 2.2 \n",
    "\n",
    "假如chunk_size=2, chunk 3的输出为:\n",
    "\n",
    "$\\left\\{\n",
    "\\begin{aligned}\n",
    "y_6 &= C_6 \\cdot A_6 \\cdot h_5 + C_6 \\cdot B_6 \\cdot x_6 \\\\\n",
    "y_7 &= C_7 \\cdot A_7 \\cdot A_6 \\cdot h_5 + C_7 \\cdot B_6 \\cdot x_6 + C_7 \\cdot B_7 \\cdot x_7\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dac14f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "526f924d",
   "metadata": {},
   "source": [
    "我们可以得到第k个chunk第i个输出的通用公式："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafbfc93",
   "metadata": {},
   "source": [
    "$ Y_{k}^i = C_{k}^i * dAcs_{k}^i * h_{k-1}^{end} + C_{k}^i * (dAcs_{k}^{i}-dAcs_k^i) * B_k^i * x_k^i$\n",
    "\n",
    "形状变化为：$[D] = [N] * [1] * [D, N] + [N] * [1] * [N] * [D]$\n",
    "\n",
    "其中，第一项在N维度上做点积求和，第二项也是在N维度上点积求和。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f177ed",
   "metadata": {},
   "source": [
    "2.1 所求的梯度是$h_{k-1}^{end}$的梯度，如果有4个chunk，输出为$[y0, y1, y2, y3, y4, y5, y6, y7]，$那么所求的梯度形状为[4, D, N]，是$[dh_{-1}, dh_1, dh_3, dh_5]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad85aafc",
   "metadata": {},
   "source": [
    "接下来就是递推状态之间的关系：\n",
    "\n",
    "$h_5 = A_5A_4 * h_3 + A_5* B_4*x_4 + B_5 * x_5$\n",
    "\n",
    "对于第k个chunk，其状态与第k-1个状态的关系为：\n",
    "\n",
    "$h_k = dAcs_k^{end} * h_{k-1} + (dAcs_k^{end} - dAcs_k) *B_k * x_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee16304a",
   "metadata": {},
   "source": [
    "形状变化为: $[D, N] = [1] * [D, N] + ([1] - [cs]) * [cs, N] * [cs, D]$\n",
    "\n",
    "第二项要在cs维度上做点积求和\n",
    "\n",
    "其中，dAcs的形状为[batch, nheads, nchunks, chunk_size]，比如第4个chunk，dAcs为[A_7, A_7*A_8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc8703f",
   "metadata": {},
   "source": [
    "kernel3就是根据kernel2输出的$[dh_{-1}, dh_1, dh_3, dh_5]$，按照倒序的顺序，求$[dh_3, dh_1, dh_-1]$，如果有Dfinal_states, 也就是$dh_7$，那就可以按照公式求解$dh_5$，否则就沿用kernel2的输出结果，如果有Dinitial_states，也就是$dh_{-1}$，那也可以根据公式求$dh_{-3}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a93fcd",
   "metadata": {},
   "source": [
    "根据前向公式，可以推导出梯度公式：\n",
    "\n",
    "$dh_{k-1} = dh_k * dAcs_k^{end} $\n",
    "\n",
    "$ddAcs_k^{end} = \\sum{dh_k * h_{k-1}}$，在D和N维度上求和，但是在kernel3中，可以把D和N维度合并为一个维度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6cdbdd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0658f190",
   "metadata": {},
   "source": [
    "### 2.3\n",
    "\n",
    "我们已经把梯度传递到了具体token（或者time）的状态中，接下来就可以把梯度从状态传递到输入x，并且求出 $\\Delta$ 和D的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110ed30c",
   "metadata": {},
   "source": [
    "#### 2.3.1 x的梯度dx\n",
    "\n",
    "dx的梯度来源于两部分，一部分来源于状态方程，一部分来源于对角线区域的输出方程：\n",
    "\n",
    "假设当前 Chunk 的长度为 $L$，输入为 $X \\in \\mathbb{R}^{L \\times D}$。为了方便矩阵表示，我们要牢记 $\\Delta$ 与 $B$ 是解耦的。我们定义中间变量（有效输入）：$$U = \\Delta \\odot X$$\n",
    "\n",
    "1. 状态生成路径 (State Equation) —— 影响“下一个 Chunk”当前 Chunk 的输入 $X$，经过 $B$ 投影和 $A$ 的衰减，最终累积成了一个状态 $h_{end}$，传给下一个 Chunk。标量形式：$$h_{end} = \\sum_{t=0}^{L-1} \\underbrace{\\exp(\\text{SegSum}(A)_{t \\to end})}_{\\text{Decay}} \\cdot B_t \\cdot (\\Delta_t x_t)$$矩阵形式：$$h_{end} = (\\mathbf{decay}_{last} \\odot B)^T \\cdot U$$\n",
    "\n",
    "2. 块内输出路径 (Intra-chunk Equation) —— 影响“当前 Chunk 输出”当前 Chunk 的输入 $X$，在块内进行因果卷积（下三角扫描），生成当前块的输出 $Y$。矩阵形式：$$Y = \\left( \\underbrace{\\mathbf{M}_{causal} \\circ \\mathbf{M}_{decay} \\circ (CB)}_{\\text{Lower Triangular Masked Matrix}} \\right) \\cdot U + D \\cdot X$$\n",
    "\n",
    "$\\mathbf{M}_{causal}$: 下三角掩码（$t \\ge k$）。\n",
    "\n",
    "$CB$: $C$ 与 $B$ 的矩阵乘积张量。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b77d39d",
   "metadata": {},
   "source": [
    "知道前向传播方程之后，求梯度就很简单了，分别求就行：\n",
    "\n",
    "求 $dx$ 的过程，就是把上述两条路径的梯度“收集”回来的过程。根据链式法则，我们先求对中间变量 $U$ 的梯度，再求对 $X$ 的梯度。定义 Accumulator (Acc) 为 Loss 对中间变量 $U$ 的梯度：$$\\text{Acc} = \\frac{\\partial \\mathcal{L}}{\\partial U}$$\n",
    "\n",
    "1. 来自状态的梯度 (From Next Chunk)对应代码 Phase 2。Loss 对 $h_{end}$ 的梯度是 dstates。梯度沿着状态生成路径倒流：$$\\text{Acc}_{state} = \\underbrace{B}_{\\text{Proj}} \\cdot \\underbrace{(\\mathbf{decay}_{last} \\odot dstates)}_{\\text{Decayed Gradient}}$$(注：代码中体现为 tl.dot(b, dstates * scale))\n",
    "\n",
    "2. 来自输出的梯度 (From Intra-chunk Output)对应代码 Phase 3。Loss 对 $Y$ 的梯度是 dout。梯度沿着块内矩阵乘法倒流。关键点：前向是 $Y = M \\cdot U$，后向则是 $\\text{Acc} = M^T \\cdot dout$。下三角矩阵 $M$ 的转置 $M^T$ 变成了上三角矩阵！$$\\text{Acc}_{scan} = \\underbrace{\\left( \\mathbf{M}_{causal}^T \\circ \\mathbf{M}_{decay}^T \\circ (CB)^T \\right)}_{\\text{Upper Triangular (Reverse Scan)}} \\cdot dout$$(注：代码中通过 stride 技巧隐式转置了 $CB$，并通过 k >= m 实现了上三角掩码)\n",
    "\n",
    "3. 最终合并：求 $dx$对应代码 Phase 4。现在我们有了总的 $\\text{Acc} = \\text{Acc}_{state} + \\text{Acc}_{scan}$。根据 $U = \\Delta \\odot X$ 和直连通路 $D \\cdot X$：$$dx = \\frac{\\partial \\mathcal{L}}{\\partial U} \\cdot \\frac{\\partial U}{\\partial X} + \\frac{\\partial \\mathcal{L}}{\\partial Y_{direct}} \\cdot \\frac{\\partial Y_{direct}}{\\partial X}$$最终矩阵公式：$$\\mathbf{dx} = \\underbrace{\\text{Acc} \\odot \\Delta}_{\\text{SSM Path}} + \\underbrace{dout \\odot D}_{\\text{Skip Path}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d114e7",
   "metadata": {},
   "source": [
    "#### 2.3.2 D和$\\Delta$的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f672103a",
   "metadata": {},
   "source": [
    "我们知道\n",
    "\n",
    "$out = SSM(x) + D*x$\n",
    "\n",
    "所以D的梯度为：\n",
    "\n",
    "$dD = \\sum(dout * x)$\n",
    "\n",
    "这里，D的维度为[nheads, head_dim]或者[nheads,]。dout的形状为[batch, seq_len, nheads, head_dim], x的形状为[batch, seq_len, nheads, head_dim]\n",
    "\n",
    "我们定义triton的grid的三个轴分别为triton.cdiv(chunk_size, META['BLOCK_SIZE_M']) * triton.cdiv(head_dim, META['BLOCK_SIZE_N']), batch * nchunks, nheads，\n",
    "\n",
    "不考虑batch，nchunks和nheads维度，对于chunk_size和head_dim中的特定Block:(pid_m, pid_n)，我们需要对dout * x的结果进行归约，如果D的维度为[nheads, head_dim]，那么就在axis=0上归约，如果为[nheads,]就全部归约。\n",
    "\n",
    "根据公式：\n",
    "\n",
    "$u = \\Delta * x$\n",
    "\n",
    "可以知道:\n",
    "\n",
    "$d\\Delta = \\sum(du * x)$\n",
    "\n",
    "其中，du就是前面求出来的acc，$d\\Delta$的形状为[batch, nchunks, nheads, chunk_size]，所以要在head_dim维度上归约。对于相同chunk_size上的Block，不同head_dim上的Block，计算出来的$d\\Delta$需要存入相同的指针地址，为了避免竞争，使用tl.atomic_add进行存储。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5fde38",
   "metadata": {},
   "source": [
    "总结一下在写这个kernel时遇到的一些高性能技巧\n",
    "\n",
    "\n",
    "1. 规约策略：空间换时间 (Scatter-Reduce / Global Accumulation)\n",
    "场景：需要计算全局梯度（如 dD），涉及 Batch、Chunk、Time 所有维度的求和。 痛点：如果成千上万个 Thread Block 同时使用 atomic_add 往同一个地址（例如 (nheads, hdim)）写入，会导致严重的原子锁竞争 (Atomic Contention)，流水线停顿，性能极差。\n",
    "\n",
    "技巧 (The dD Trick)：\n",
    "\n",
    "分两步走：\n",
    "\n",
    "Kernel 内（Scatter）：给每个计算单元（Grid 节点）分配一块“私有显存”。\n",
    "\n",
    "定义一个高维 Tensor（例如 5D），包含 [Num_Time_Blocks, Batch, Chunk, ...]。\n",
    "\n",
    "每个 Block 只写自己的位置，完全无锁（Lock-free），使用极快的 store。\n",
    "\n",
    "Kernel 外（Reduce）：利用 PyTorch 的高度优化算子进行求和。\n",
    "\n",
    "dD.sum(dim=(0, 1, 2))。\n",
    "\n",
    "核心思想：通过增加显存占用（空间），消除原子锁竞争（时间），大幅提升吞吐量。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7f36e1",
   "metadata": {},
   "source": [
    "2. 并发安全：原子加 vs. 直接存 (Atomic Add vs. Store)\n",
    "场景：多个 Block 需要写入同一个输出 Tensor。 法则：看 Grid 切分了哪个维度，以及我们在哪个维度上求和。\n",
    "\n",
    "比如在求ddt时，使用了\n",
    "\n",
    "ddt = tl.sum(acc * x, axis=1)\n",
    "\n",
    "acc和x都是在chunk_size和head_dim维度上进行切分的，ddt在head_dim维度上进行了求和，而ddt的存储地址为\n",
    "\n",
    "ddt_ptrs = ddt_ptr + offs_m * stride_ddt_csize\n",
    "\n",
    "也就是说，对于相同的offs_m偏移时，不同的offs_n偏移的kernel会同时往ddt_ptrs地址中存数据，此时就需要要原子加，在其中一个kernel访问该地址时，其他kernel要等候。\n",
    "\n",
    "而对于常规kernel的数据存储地址，每个kernel的存储地址不同，所以不存在竞争的问题，此时用tl.store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed321d94",
   "metadata": {},
   "source": [
    "3. 访存魔法：零开销转置 (Stride Trick / Zero-overhead Transpose)场景：计算矩阵乘法 $Acc += CB^T \\cdot dout$。物理内存中存的是 $CB$，但我们需要 $CB^T$。痛点：在 Kernel 里手动交换索引（val[k, m]）写起来麻烦，且显式转置数据非常慢。技巧：修改元数据，不动数据：在 Python 端调用 Kernel 时，交换传入的 Stride 参数。传给 Kernel 的 stride_row其实是物理的 stride_col。传给 Kernel 的 stride_col其实是物理的 stride_row。效果：Kernel 代码依然写 ptr + m*stride_m + k*stride_k，但硬件读出来的数据已经是转置好的。核心思想：利用 View（视图）的灵活性，避免 Data Movement（数据搬运）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d55294d",
   "metadata": {},
   "source": [
    "4. 计算优化：指针跳跃 (Pointer Jumping for Causal Mask)场景：处理因果掩码（Causal Mask, $k \\ge m$）或三角矩阵。痛点：朴素做法是从 $k=0$ 循环到 $K$，然后在循环体内用 if k < m: mask=0。这导致大量的无效内存读取和无效计算（乘 0）。技巧：直接跳过无效区：利用 Grid 信息知道当前 Block 负责的起始时间 $m_{start}$ (K_MIN)。初始化指针时，直接 ptr += K_MIN * stride。循环直接从 K_MIN 开始。核心思想：只计算有效数据。对于下三角/上三角计算，这能节省接近 50% 的算力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e434cffe",
   "metadata": {},
   "source": [
    "5. 数值稳定性：对数空间的安全操作场景：计算指数衰减 $\\exp(\\text{cumsum}_k - \\text{cumsum}_m)$。技巧：钳位 (Clamping)：tl.exp(tl.minimum(val, 0.0))。原因：虽然理论上 $k \\ge m$ 时差值应 $\\le 0$，但浮点误差可能导致微小的正数，导致 exp 结果 $>1$（梯度爆炸）。强制截断到 0 保证数值安全。\n",
    "\n",
    "6. 维度切分：Tiling (分块)场景：特征维度 headdim 或状态维度 dstate 很大，单个 Block 的寄存器/Shared Memory 放不下。技巧：Grid 切分：不仅切分 Batch 和 Time，还切分 Feature (pid_n)。循环切分：在 Kernel 内部，对 dstate 维度 (K) 进行循环迭代 (for k in range(0, dstate, BLOCK_K))。核心思想：分而治之。无论 Tensor 多大，都把它切成适合 GPU 硬件单元（Tensor Core / SRAM）大小的小块来处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b9e580",
   "metadata": {},
   "source": [
    "### 2.4 B的梯度\n",
    "\n",
    "同样是根据状态方程：\n",
    "\n",
    "$$h_{end} = (\\mathbf{decay}_{t->last} \\odot B)^T \\cdot U$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a93d18",
   "metadata": {},
   "source": [
    "求得B的梯度：\n",
    "\n",
    "$$dB = dh_{end} * decay_{t->end} * U$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9ec44f",
   "metadata": {},
   "source": [
    "dB的形状为[batch, seq_len, ngroups, d_state]，dstates为[batch, nchunks, nheads, head_dim, d_state], U为[batch, seq_len, nheads, head_dim]\n",
    "\n",
    "grid的划分为：triton.cdiv(chunk_size, META['BLOCK_SIZE_M']) * triton.cdiv(dstate, META['BLOCK_SIZE_N']), batch * nchunks, nsplits * ngroups\n",
    "\n",
    "不考虑batch和nchunks以及nheads，U: [cs, head_dim], dstates: [head_dim, d_state]\n",
    "\n",
    "所以，grid的第一个轴是根据chunk_size和d_state进行划分的。\n",
    "\n",
    "除此之外，这里需要注意GQA机制，也就是说ngroups对应nheads，一个group对应nheads // ngroups个头。我们把同一个group下的nheads划分为多个Blocks，也就是split个block，一共有ngroups * split个block，同一个block中，把该block中的nheads的dB相加。\n",
    "\n",
    "除此之外，还需要求decay的梯度\n",
    "\n",
    "$$decay_{t->end} = exp(L_{end} - L_t)$$\n",
    "\n",
    "所以：\n",
    "\n",
    "$$d\\mathbf{decay}_{t->end} = \\sum(dB * B)$$\n",
    "\n",
    "在d_state维度上归约，注意，这里再进行保存时，需要使用atomic_add。并且后移一位保存。\n",
    "\n",
    "比如对于一个chunk内由[0, 1, 2, 3]个token，此时求出来的$d\\mathbf{decay}_{t->end}$分别对应4个token，保存时，只保存[1, 2, 3]位置处的，其中0处的不保存，就是0！\n",
    "\n",
    "到这里，decay梯度的求解还未完成！\n",
    "\n",
    "我们看，求出来的decay的梯度分别是什么：\n",
    "\n",
    "$$[\\partial{exp(L_3 - L0)}, \\partial{exp(L_3 - L1)}, \\partial{exp(L_3 - L2)}, \\partial{exp(L_3 - L3)}]$$\n",
    "\n",
    "也就是\n",
    "\n",
    "$$[\\partial{A1+A2+A3}, \\partial{A2+A3}, \\partial{A3}, \\partial{0}]$$\n",
    "\n",
    "我们在保存时，只保存了前三位，分别保存在了[1, 2, 3]处\n",
    "\n",
    "$$[0, \\partial{A1+A2+A3}, \\partial{A2+A3}, \\partial{A3}]$$\n",
    "\n",
    "假如我们想求$[\\partial{A0}, \\partial{A1}, \\partial{A2}, \\partial{A3}]$，可以发现，A0没有梯度回传，A1只与1有关，A2与1和2有关，A3与123有关\n",
    "\n",
    "可以求出：\n",
    "\n",
    "$$[0, \\partial{A1+A2+A3}, \\partial{A1+A2+A3} + \\partial{A2+A3}, \\partial{A1+A2+A3} + \\partial{A2+A3} + \\partial{A3}]$$\n",
    "\n",
    "也就是在\n",
    "\n",
    "$$[0，\\partial{A1+A2+A3}, \\partial{A2+A3}, \\partial{A3}, \\partial{0}]$$\n",
    "\n",
    "的基础上进行cumsum！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fade8236",
   "metadata": {},
   "source": [
    "### 2.5 C的梯度\n",
    "\n",
    "求C的梯度有两部分来源，一部分是非对角线块中的左侧项，这部分是根据当前chunk的上一个chunk结束的状态，求出当前chunk的输出。前向传播公式如下：\n",
    "\n",
    "$$\n",
    "Y = C * h_{end}^T * dAcs_{end->t}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d352a310",
   "metadata": {},
   "source": [
    "其中$h_{end}$是上一个chunk的结束状态，$dAcs$是当前chunk的衰减项。\n",
    "\n",
    "得出$C$的梯度如下：\n",
    "\n",
    "$$\n",
    "dC = dY * h_{end} * dAcs_{end->t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5546df25",
   "metadata": {},
   "source": [
    "可以求出，$dAcs$的梯度如下：\n",
    "\n",
    "$$\n",
    "dAcs_{end->t} = \\sum{dC * C}\n",
    "$$\n",
    "\n",
    "在N维度上归约"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c917d9",
   "metadata": {},
   "source": [
    "### 2.6 求dCB的梯度\n",
    "\n",
    "根据对角线块内公式：\n",
    "\n",
    "$$\n",
    "Y = CB * dAcs * \\Delta * x\n",
    "$$\n",
    "\n",
    "可以求出，$dCB$的梯度如下：\n",
    "\n",
    "$$d\\mathbf{CB} = \\left( (\\mathbf{dout} \\times \\mathbf{x}^T) \\odot \\mathbf{\\Delta}_{\\text{broad}} \\odot \\mathbf{S}_{\\text{decay}} \\right) \\odot \\mathbf{M}_{\\text{causal}}$$\n",
    "\n",
    "$\\mathbf{dout} \\times \\mathbf{x}^T$: 外积 (Outer Product)。\n",
    "\n",
    "$Query \\times Key^T$。这是梯度最原始的来源。\n",
    "\n",
    "维度：$[L, D] \\times [D, L] \\rightarrow [L, L]$\n",
    "\n",
    "$\\odot \\mathbf{\\Delta}$: 乘上步长 $dt$。\n",
    "\n",
    "$\\odot \\mathbf{S}_{\\text{decay}}$: 乘上衰减项 $\\exp(L_t - L_k)$。\n",
    "\n",
    "$\\odot \\mathbf{M}_{\\text{causal}}$: 掩码操作（下三角保留，右上角置零）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bbcd9a",
   "metadata": {},
   "source": [
    "### 2.7 根据dCB求dC和dB\n",
    "\n",
    "这里其实就是一个通用矩阵乘法的梯度公式：\n",
    "\n",
    "$$\n",
    "CB = C * B^T\n",
    "$$\n",
    "\n",
    "可以求出：\n",
    "\n",
    "$$\n",
    "dB = dCB^T * C\n",
    "$$\n",
    "\n",
    "$$\n",
    "dC = dCB * B\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151eeb31",
   "metadata": {},
   "source": [
    "### 2.8 ddA\n",
    "\n",
    "我们前面已经求解了三个关于ddA的结果：ddA_chunk_cumsum, ddA_cumsum_prev, ddA_next。分别来源于以下公式：\n",
    "\n",
    "$$h_k = dAcs_k^{end} * h_{k-1} + (dAcs_k^{end} - dAcs_k) *B_k * x_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c184965a",
   "metadata": {},
   "source": [
    "其中,$dAcs_k^{end}=exp(L_k^{end} - L_{k-1}^{end})$，当前chunk结尾和上一个chunk结尾之间的衰减。ddA_chunk_cumsum就是这个衰减的梯度。其形状是[nchunks]的：\n",
    "\n",
    "$$[dexp(A0 + A1), dexp(A2 + A3), dexp(A4 + A5), ...]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc160ce9",
   "metadata": {},
   "source": [
    "在求解dB时，我们也可以把上式中的$(dAcs_k^{end} - dAcs_k)$的梯度求解出来，也就是ddA_next。其形状是[nchunks, chunk_size]：\n",
    "\n",
    "$$[[dexp(A0), dexp(A1)], [dexp(A2), dexp(A3)], [dexp(A4), dexp(A5)], ...]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d48261",
   "metadata": {},
   "source": [
    "在求解dC时，可以求出ddA_cumsum_prev，根据公式：\n",
    "\n",
    "$$\n",
    "Y = C * h_{end}^T * dAcs_{end->t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b231fd5f",
   "metadata": {},
   "source": [
    "其形状为：[nchunks, chunk_size]:\n",
    "\n",
    "$$\n",
    "[[dexp(A0), dexp(A0 + A1)], [dexp(A2), dexp(A2 + A3)], [dexp(A4), dexp(A4 + A5)], ...]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d224164a",
   "metadata": {},
   "source": [
    "将ddA_cumsum_prev和ddA_chunk_cumsum合并：\n",
    "\n",
    "ddA_cumsum_prev[..., -1] += ddA_chunk_cumsum\n",
    "\n",
    "可以看出，ddA_cumsum_prev中其实是每个chunk内部dA的cumsum:\n",
    "\n",
    "$$\n",
    "[[dexp(A0), dexp(A0 + A1)], [dexp(A2), dexp(A2 + A3)], [dexp(A4), dexp(A4 + A5)], ...] \\\\\n",
    "\n",
    "= \\\\\n",
    "\n",
    "[[dexp(A0), dexp(A1)], [dexp(A2), dexp(A3)], [dexp(A4), dex(A4)],...].cumsum(dim=-1)\n",
    "$$\n",
    "\n",
    "$y=cumsum(x)$的梯度公式为：dx=flip(cumsum(flip(dy)))，所以：\n",
    "\n",
    "$$\n",
    "[[dexp(A0), dexp(A1)], [dexp(A2), dexp(A3)], [dexp(A4), dex(A4)],...] \\\\ = \\\\ ddA_{prev} = \\text{ddA\\_chunk\\_cumsum}.flip[-1].cumsum(dim=-1).flip(-1)\n",
    "$$\n",
    "\n",
    "注意到，ddA_next其实就是$[[dexp(A0), dexp(A1)], [dexp(A2), dexp(A3)], [dexp(A4), dex(A4)],...]$,这样就可以把ddA_prev和ddA_next合并了！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98816e3c",
   "metadata": {},
   "source": [
    "接下来需要根据对角线块，也就是chunk内部的公式，求ddA！！难点\n",
    "\n",
    "该部分的前向传播式为：\n",
    "\n",
    "$$\n",
    "Y = CB * dt * Decay * x \\\\\n",
    "\n",
    "y_t = C_t * \\sum_{s=0}^{t} B_s^T * dt_s * exp(L_t - L_s) * x_s\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaf5469",
   "metadata": {},
   "source": [
    "其中$exp(L_t - L_s) = exp(A_{s+1} + A_{s+2} + ... + A_t)$，假如t足够大，s从0开始，decay分别为：\n",
    "\n",
    "$$\n",
    "s=0: exp(A_1 + A_2 + A_3 + ... ) \\\\\n",
    "\n",
    "s=1: exp(A_2 + A_3 + A_3 + ... ) \\\\\n",
    "\n",
    "s=2: exp(A_3 + A_4 + A_5 + ... ) \\\\\n",
    "\n",
    "s=3: exp(A_4 + A_5 + A_6 + ... ) \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    "s=t-1: exp(A_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d49db3",
   "metadata": {},
   "source": [
    "以上在进行求导时，分别能够求得：\n",
    "\n",
    "$$\n",
    "s=0: dA_1, dA_2, dA_3 \\\\\n",
    "s=1: dA_2, dA_3, dA_4 \\\\\n",
    "s=2: dA_3, dA_4, dA_5 \\\\\n",
    "s=3: dA_4, dA_5, dA_6 \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    "s=t-1: dA_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e20a09e",
   "metadata": {},
   "source": [
    "这是一个长度为t的数组，我们需要进行cumsum分别得到$[dA_1, dA_2, dA_3, ..., dA_t]$的值\n",
    "\n",
    "这还只是第t行，第t+1行可以求得$[dA_1, dA_2, dA_3, ..., dA_{t+1}]$，第t+2行可以求得：$[dA_1, dA_2, dA_3, ..., dA_{t+2}]$\n",
    "\n",
    "以此类推是一个下三角矩阵，不同的行的同一列贡献的是同一个A的导数，所以也需要在行方向上cumsum。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e561f58",
   "metadata": {},
   "source": [
    "根据前向传播公式：\n",
    "\n",
    "$$\n",
    "Y = CB * dt * Decay * x\n",
    "$$\n",
    "\n",
    "可以求出矩阵形式的反向传播公式:\n",
    "\n",
    "$$\n",
    "d_{Decay} = dY @ x^T * CB * dt * Decay\n",
    "$$\n",
    "\n",
    "注意，这里的$d_{Decay}$是对具体指数部分的梯度，不是对exp(...)的梯度，因为最后乘了一个Decay!!!!\n",
    "\n",
    "在编写kernel时，只需要对$dY @ x^T * CB * dt * Decay$这个矩阵进行行和列的分块即可。但是中间也需要对head_dim维度进行分块。\n",
    "\n",
    "假如当前kernel负责pid_m行，那么需要在[0, (pid_m + 1) * BLOCK_SIZE_M]中进行列的循环，因为必须满足行 > 列。\n",
    "\n",
    "循环内每次求BLOCK_SIZE_N个列，求完之后进行cumsum，由于每次循环求的是当前列块的cumsum，所以每次循环中要加上一个列块的sum值。循环结束后，就求出了当前pid_m个行BLOCK所对应的梯度值。\n",
    "\n",
    "循环内部需要进行mask，条件就是offs_m[:, None] > offs_n[None, :] + start_n\n",
    "\n",
    "最后，在行维度上进行sum，也就是把当前BLOCK的所有行的值加起来。\n",
    "\n",
    "需要注意的是，$\\Delta A_0$是0，当前列s求得的梯度对应的是$\\Delta A_{s+1}$的梯度，所以需要后移一位进行存储！\n",
    "\n",
    "最后kernel结束后，求出来的是[chunk_size // BLOCK_SIZE_M, chunk_size]的ddA，需要对第一个维度进行求和。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0ba693",
   "metadata": {},
   "source": [
    "### 2.9 求ddt, dA, dt_bias\n",
    "\n",
    "我们已经求出$ddA$：\n",
    "\n",
    "$$\n",
    "dA = A * dt\n",
    "$$\n",
    "\n",
    "所以可以得出ddt和dA：\n",
    "\n",
    "$$\n",
    "ddt_{final} = ddA * A + ddt_{else}\\\\\n",
    "\n",
    "dA = \\sum_{batch, nchunks, chunksize} ddA * dt\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942ddc00",
   "metadata": {},
   "source": [
    "如果输入的dt已经经过softplus激活了，那么就不需要继续往下进行梯度回传了，pytoch会自动进行梯度回传。\n",
    "\n",
    "如果输入的dt是原始dt参数，那么还需要继续进行梯度回传。上面的dt实际上是:\n",
    "\n",
    "$$\n",
    "dt_{final} = clamp(softplus(dt + dt_{bias}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82444fb",
   "metadata": {},
   "source": [
    "我们上面的到ddt还需要求：\n",
    "\n",
    "1. clamp的梯度:\n",
    "\n",
    "$$\n",
    "grad = clamp((dt < dt_{min}) | (dt > dt_{max}), 0.0, ddt_{final})\n",
    "$$\n",
    "\n",
    "2. softplus的梯度：\n",
    "\n",
    "$$\n",
    "ddt = ddt_{final} * sigmoid(dt + dt_{bias}) \\\\\n",
    "\n",
    "\\text{softplus}'(x) = \\text{sigmoid}(x)\n",
    "$$\n",
    "\n",
    "3. 关于偏置：dt_bias的梯度：\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\text{bias}} = \\sum_{\\text{batch}, nchunks, chunksize} \\frac{\\partial \\mathcal{L}}{\\partial \\Delta t_{\\text{input}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62752f95",
   "metadata": {},
   "source": [
    "至此，完成了Mamba2的kernel编写工作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a444b70",
   "metadata": {},
   "source": [
    "# Forward Kernel Figure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915d5bbe",
   "metadata": {},
   "source": [
    "![Forward Kernel Figure](images/fwd_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1bbf8b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
